{"config":{"lang":["en","zh"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Guides/outfit_with_loop/","title":"Outfit Recommendation with Loop Example","text":"<p>This example demonstrates how to use the framework for outfit recommendation tasks with loop functionality. The example code can be found in the <code>examples/step3_outfit_with_loop</code> directory.</p> <pre><code>   cd examples/step3_outfit_with_loop\n</code></pre>"},{"location":"Guides/outfit_with_loop/#overview","title":"Overview","text":"<p>This example implements an interactive outfit recommendation workflow that uses a loop-based approach to refine recommendations based on user feedback. The workflow consists of the following key components:</p> <ol> <li>Initial Image Input</li> <li>OutfitImageInput: Handles the upload and processing of the initial clothing item image</li> <li> <p>Serves as the starting point for the recommendation process</p> </li> <li> <p>Interactive QA Loop with Weather Integration</p> </li> <li>OutfitQA: Conducts an interactive Q&amp;A session to gather context and preferences</li> <li>Uses web search tool to fetch real-time weather data for the specified location</li> <li>OutfitDecider: Evaluates if sufficient information has been collected based on:<ul> <li>User preferences</li> <li>Current weather conditions</li> </ul> </li> <li>Uses DoWhileTask to continue the loop until adequate information is gathered</li> <li> <p>Loop terminates when OutfitDecider returns decision=true</p> </li> <li> <p>Final Recommendation</p> </li> <li> <p>OutfitRecommendation: Generates the final outfit suggestions based on:</p> <ul> <li>The initial uploaded image</li> <li>Information collected during the Q&amp;A loop</li> <li>Current weather conditions from web search</li> <li>Other context (occasion, preferences, etc.)</li> </ul> </li> <li> <p>Workflow Flow <code>Start -&gt; Image Input -&gt; OutfitQA Loop (QA + Weather Search + Decision) -&gt; Final Recommendation -&gt; End</code></p> </li> </ol> <p>The workflow leverages Redis for state management and the Conductor server for workflow orchestration. This architecture enables: - Image-based outfit recommendations - Weather-aware outfit suggestions using real-time data - Interactive refinement through structured Q&amp;A - Context-aware suggestions incorporating multiple factors - Persistent state management across the workflow</p>"},{"location":"Guides/outfit_with_loop/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Required packages installed (see requirements.txt)</li> <li>Access to OpenAI API or compatible endpoint</li> <li>Access to Bing API key for web search functionality to search real-time weather information for outfit recommendations (see configs/tools/websearch.yml)</li> <li>Redis server running locally or remotely</li> <li>Conductor server running locally or remotely</li> </ul>"},{"location":"Guides/outfit_with_loop/#configuration","title":"Configuration","text":"<p>The container.yaml file is a configuration file that manages dependencies and settings for different components of the system, including Conductor connections, Redis connections, and other service configurations. To set up your configuration:</p> <ol> <li> <p>Generate the container.yaml file:    <code>bash    python compile_container.py</code>    This will create a container.yaml file with default settings under <code>examples/step3_outfit_with_loop</code>.</p> </li> <li> <p>Configure your LLM settings in <code>configs/llms/gpt.yml</code> and <code>configs/llms/text_res.yml</code>:</p> </li> <li>Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file    <code>bash    export custom_openai_key=\"your_openai_api_key\"    export custom_openai_endpoint=\"your_openai_endpoint\"</code></li> <li> <p>Configure other model settings like temperature as needed through environment variable or by directly modifying the yml file</p> </li> <li> <p>Configure your Bing Search API key in <code>configs/tools/websearch.yml</code>:</p> </li> <li> <p>Set your Bing API key through environment variable or by directly modifying the yml file    <code>bash    export bing_api_key=\"your_bing_api_key\"</code></p> </li> <li> <p>Update settings in the generated <code>container.yaml</code>:</p> </li> <li>Modify Redis connection settings:<ul> <li>Set the host, port and credentials for your Redis instance</li> <li>Configure both <code>redis_stream_client</code> and <code>redis_stm_client</code> sections</li> </ul> </li> <li>Update the Conductor server URL under conductor_config section</li> <li>Adjust any other component settings as needed</li> </ol>"},{"location":"Guides/outfit_with_loop/#running-the-example","title":"Running the Example","text":"<ol> <li>Run the outfit recommendation workflow:</li> </ol> <p>For terminal/CLI usage:    <code>bash    python run_cli.py</code></p> <p>For app/GUI usage:    <code>bash    python run_app.py</code></p>"},{"location":"Guides/outfit_with_loop/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues: - Verify Redis is running and accessible - Check your OpenAI API key and Bing API key are valid - Ensure all dependencies are installed correctly - Review logs for any error messages - Confirm Conductor server is running and accessible - Check Redis Stream client and Redis STM client configuration</p>"},{"location":"Guides/outfit_with_loop/#building-the-example","title":"Building the Example","text":"<p>Coming soon! This section will provide detailed instructions for building the step3_outfit_with_loop example step by step.</p>"},{"location":"Guides/outfit_with_ltm/","title":"Outfit Recommendation with Long-Term Memory Example","text":"<p>This example demonstrates how to use the framework for outfit recommendation tasks with long-term memory functionality. The example code can be found in the <code>examples/step4_outfit_with_ltm</code> directory.</p> <pre><code>   cd examples/step4_outfit_with_ltm\n</code></pre>"},{"location":"Guides/outfit_with_ltm/#overview","title":"Overview","text":"<p>This example implements an outfit recommendation system with long-term memory capabilities through two main workflows:</p> <ol> <li>Image Storage Workflow</li> <li>ImageIndexListener: Monitors and captures new clothing images</li> <li>OutfitImagePreprocessor: Processes and prepares images for storage</li> <li>Stores processed images in Milvus long-term memory (LTM) for future retrieval</li> <li> <p>Workflow sequence: Image Listening -&gt; Preprocessing -&gt; LTM Storage</p> </li> <li> <p>Outfit Recommendation Workflow</p> </li> <li>OutfitQA: Conducts interactive Q&amp;A to understand user preferences</li> <li>OutfitDecider: Determines if sufficient information is collected</li> <li>Uses DoWhileTask for iterative refinement until decision is positive</li> <li>OutfitGeneration: Generates outfit recommendations using stored image data</li> <li>OutfitConclusion: Presents final recommendations with explanations</li> </ol> <p>The system leverages both short-term memory (Redis STM) and long-term memory (Milvus LTM) for: - Efficient image storage and retrieval - Persistent clothing item database - Context-aware outfit recommendations - Interactive preference refinement - Stateful conversation management</p> <ol> <li>Workflow Architecture <code>Image Storage:    Listen -&gt; Preprocess -&gt; Store in LTM    Recommendation:   QA Loop (QA + Decision) -&gt; Generation -&gt; Conclusion</code></li> </ol> <p>The system uses Redis for state management, Milvus for long-term image storage, and Conductor for workflow orchestration. This architecture enables: - Scalable image database management - Intelligent outfit recommendations based on stored items - Interactive preference gathering - Persistent clothing knowledge base - Efficient retrieval of relevant items</p>"},{"location":"Guides/outfit_with_ltm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Required packages installed (see requirements.txt)</li> <li>Access to OpenAI API or compatible endpoint (see configs/llms/gpt.yml)</li> <li>Access to Bing API key for web search functionality to search real-time weather information for outfit recommendations (see configs/tools/websearch.yml)</li> <li>Redis server running locally or remotely</li> <li>Conductor server running locally or remotely</li> <li>Milvus vector database (will be started automatically when workflow runs)</li> <li>Sufficient storage space for image database</li> <li>Install Git LFS by <code>git lfs intall</code>, then pull sample images by <code>git lfs pull</code></li> </ul>"},{"location":"Guides/outfit_with_ltm/#configuration","title":"Configuration","text":"<p>The container.yaml file is a configuration file that manages dependencies and settings for different components of the system, including Conductor connections, Redis connections, Milvus connections and other service configurations. To set up your configuration:</p> <ol> <li>Generate the container.yaml files:    ```bash    # For image storage workflow    python image_storage/compile_container.py</li> </ol> <p># For outfit recommendation workflow    python outfit_from_storage/compile_container.py    <code>``    This will create two container.yaml files with default settings under</code>image_storage<code>and</code>outfit_from_storage<code>directories:    -</code>image_storage/container.yaml<code>: Configuration for the image storage workflow    -</code>outfit_from_storage/container.yaml`: Configuration for the outfit recommendation workflow</p> <ol> <li>Configure your LLM settings in <code>configs/llms/gpt.yml</code> and <code>configs/llms/text_res.yml</code> in the two workflow directories:</li> <li>Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file    <code>bash    export custom_openai_key=\"your_openai_api_key\"    export custom_openai_endpoint=\"your_openai_endpoint\"</code></li> <li> <p>Configure other model settings like temperature as needed through environment variable or by directly modifying the yml file</p> </li> <li> <p>Configure your Bing Search API key in <code>configs/tools/websearch.yml</code> in the two workflow directories:</p> </li> <li>Set your Bing API key through environment variable or by directly modifying the yml file    <code>bash    export bing_api_key=\"your_bing_api_key\"</code></li> <li>Configure your text encoder settings in <code>configs/llms/text_encoder.yml</code> in the two workflow directories:</li> <li>Set your OpenAI text encoder endpoint and API key through environment variable or  by directly modifying the yml file    <code>bash    export custom_openai_text_encoder_key=\"openai_text_encoder_key\"    export custom_openai_text_encoder_endpoint=\"your_openai_endpoint\"</code></li> <li>The default text encoder configuration uses OpenAI text embedding v3 with 3072 dimensions, make sure you change the dim value of <code>MilvusLTM</code> in <code>container.yaml</code></li> <li> <p>Adjust the embedding dimension and other settings as needed through environment variable or by directly modifying the yml file</p> </li> <li> <p>Update settings in the generated <code>container.yaml</code>:</p> </li> <li>Modify Redis connection settings:<ul> <li>Set the host, port and credentials for your Redis instance</li> <li>Configure both <code>redis_stream_client</code> and <code>redis_stm_client</code> sections</li> </ul> </li> <li>Update the Conductor server URL under conductor_config section</li> <li>Configure MilvusLTM in <code>components</code> section:<ul> <li>Set the <code>storage_name</code> and <code>dim</code> for MilvusLTM</li> <li>Adjust other settings as needed</li> </ul> </li> <li>Adjust any other component settings as needed</li> </ol>"},{"location":"Guides/outfit_with_ltm/#running-the-example","title":"Running the Example","text":"<ol> <li>Run the image storage workflow first:</li> </ol> <p>For terminal/CLI usage:    <code>bash    python image_storage/run_image_storage_cli.py</code>    For app usage:    <code>bash    python image_storage/run_image_storage_app.py</code></p> <p>This workflow will store outfit images in the Milvus database.</p> <ol> <li>Run the outfit recommendation workflow in a separate terminal:</li> </ol> <p>For terminal/CLI usage:    <code>bash    python outfit_from_storage/run_outfit_recommendation_cli.py</code></p> <p>For app/GUI usage:    <code>bash    python outfit_from_storage/run_outfit_recommendation_app.py</code></p> <p>This workflow will retrieve outfit recommendations from the stored images.</p>"},{"location":"Guides/outfit_with_ltm/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues: - Verify Redis is running and accessible - Check your OpenAI API key and Bing API key are valid - Ensure all dependencies are installed correctly - Review logs for any error messages - Confirm Conductor server is running and accessible - Check Redis Stream client and Redis STM client configuration</p>"},{"location":"Guides/outfit_with_ltm/#building-the-example","title":"Building the Example","text":"<p>Coming soon! This section will provide detailed instructions for building the step4_outfit_with_ltm example step by step.</p>"},{"location":"Guides/outfit_with_switch/","title":"Outfit Recommendation with Switch Example","text":"<p>This example demonstrates how to use the framework for outfit recommendation tasks with switch_case functionality. The example code can be found in the <code>examples/step2_outfit_with_switch</code> directory.</p> <pre><code>   cd examples/step2_outfit_with_switch\n</code></pre>"},{"location":"Guides/outfit_with_switch/#overview","title":"Overview","text":"<p>This example implements an outfit recommendation workflow that uses switch-case functionality to conditionally include weather information in the recommendation process. The workflow consists of the following key components:</p> <ol> <li>Input Interface</li> <li>Handles user input containing clothing requests and image data</li> <li>Processes and caches any uploaded images</li> <li> <p>Extracts the user's outfit request instructions</p> </li> <li> <p>Weather Decision Logic</p> </li> <li>WeatherDecider: Analyzes the user's request to determine if weather information is needed</li> <li>Makes a binary decision (0 or 1) based on context in the user's request</li> <li> <p>Controls whether weather data should be fetched</p> </li> <li> <p>Conditional Weather Search</p> </li> <li>WeatherSearcher: Only executes if WeatherDecider returns 0 (weather info needed)</li> <li>Uses web search functionality to fetch current weather conditions</li> <li> <p>Integrates weather data into the recommendation context</p> </li> <li> <p>Outfit Recommendation</p> </li> <li>Generates final clothing suggestions based on:<ul> <li>User's original request</li> <li>Weather information (if available)</li> <li>Any provided image context</li> </ul> </li> <li>Provides complete outfit recommendations</li> </ol> <p>The workflow follows this sequence:</p>"},{"location":"Guides/outfit_with_switch/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Required packages installed (see requirements.txt)</li> <li>Access to OpenAI API or compatible endpoint (see configs/llms/gpt.yml)</li> <li>Access to Bing API key for web search functionality to search real-time weather information for outfit recommendations (see configs/tools/websearch.yml)</li> <li>Redis server running locally or remotely</li> <li>Conductor server running locally or remotely</li> </ul>"},{"location":"Guides/outfit_with_switch/#configuration","title":"Configuration","text":"<p>The container.yaml file is a configuration file that manages dependencies and settings for different components of the system, including Conductor connections, Redis connections, and other service configurations. To set up your configuration:</p> <ol> <li> <p>Generate the container.yaml file:    <code>bash    python compile_container.py</code>    This will create a container.yaml file with default settings under <code>examples/step2_outfit_with_switch</code>.</p> </li> <li> <p>Configure your LLM settings in <code>configs/llms/gpt.yml</code> and <code>configs/llms/text_res.yml</code>:</p> </li> <li> <p>Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file    <code>bash    export custom_openai_key=\"your_openai_api_key\"    export custom_openai_endpoint=\"your_openai_endpoint\"</code></p> </li> <li> <p>Configure other model settings like temperature as needed through environment variable or by directly modifying the yml file</p> </li> <li> <p>Configure your Bing Search API key in <code>configs/tools/websearch.yml</code>:</p> </li> <li> <p>Set your Bing API key through environment variable or by directly modifying the yml file    <code>bash    export bing_api_key=\"your_bing_api_key\"</code></p> </li> <li> <p>Update settings in the generated <code>container.yaml</code>:</p> </li> <li>Modify Redis connection settings:<ul> <li>Set the host, port and credentials for your Redis instance</li> <li>Configure both <code>redis_stream_client</code> and <code>redis_stm_client</code> sections</li> </ul> </li> <li>Update the Conductor server URL under conductor_config section</li> <li>Adjust any other component settings as needed</li> </ol>"},{"location":"Guides/outfit_with_switch/#running-the-example","title":"Running the Example","text":"<ol> <li>Run the outfit recommendation with switch example:</li> </ol> <p>For terminal/CLI usage:    <code>bash    python run_cli.py</code></p> <p>For app/GUI usage:    <code>bash    python run_app.py</code></p>"},{"location":"Guides/outfit_with_switch/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ul> <li>Verify Conductor and Redis are running and accessible</li> <li>Check your OpenAI API key and Bing API key are valid</li> <li> <p>Check Redis Stream client and Redis STM client configuration</p> </li> <li> <p>Ensure all dependencies are installed correctly</p> </li> <li>Review logs for any error messages</li> </ul>"},{"location":"Guides/outfit_with_switch/#building-the-example","title":"Building the Example","text":"<p>Coming soon! This section will provide detailed instructions for building the step2_outfit_with_switch example step by step.</p>"},{"location":"Guides/simple_vqa/","title":"Simple Visual Question Answering Example","text":"<p>This example demonstrates how to use the framework for visual question answering (VQA) tasks. The example code can be found in the <code>examples/step1_simpleVQA</code> directory.</p> <pre><code>   cd examples/step1_simpleVQA\n</code></pre>"},{"location":"Guides/simple_vqa/#overview","title":"Overview","text":"<p>This example implements a simple Visual Question Answering (VQA) workflow that consists of two main components:</p> <ol> <li>Input Interface</li> <li>Handles user input containing questions about images</li> <li>Processes and manages image data</li> <li> <p>Extracts the user's questions/instructions</p> </li> <li> <p>Simple VQA Processing</p> </li> <li>Takes the user input and image</li> <li>Analyzes the image based on the user's question</li> <li>Generates appropriate responses to visual queries</li> </ol> <p>The workflow follows a straightforward sequence:</p>"},{"location":"Guides/simple_vqa/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Required packages installed (see requirements.txt)</li> <li>Access to OpenAI API or compatible endpoint (see configs/llms/gpt.yml)</li> <li>Redis server running locally or remotely</li> <li>Conductor server running locally or remotely</li> </ul>"},{"location":"Guides/simple_vqa/#configuration","title":"Configuration","text":"<p>The container.yaml file is a configuration file that manages dependencies and settings for different components of the system, including Conductor connections, Redis connections, and other service configurations. To set up your configuration:</p> <ol> <li> <p>Generate the container.yaml file:    <code>bash    python compile_container.py</code>    This will create a container.yaml file with default settings under <code>examples/step1_simpleVQA</code>.</p> </li> <li> <p>Configure your LLM settings in <code>configs/llms/gpt.yml</code>:</p> </li> <li>Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file    <code>bash    export custom_openai_key=\"your_openai_api_key\"    export custom_openai_endpoint=\"your_openai_endpoint\"</code></li> <li> <p>Configure other model settings like temperature as needed through environment variable or by directly modifying the yml file</p> </li> <li> <p>Update settings in the generated <code>container.yaml</code>:</p> </li> <li>Modify Redis connection settings:<ul> <li>Set the host, port and credentials for your Redis instance</li> <li>Configure both <code>redis_stream_client</code> and <code>redis_stm_client</code> sections</li> </ul> </li> <li>Update the Conductor server URL under conductor_config section</li> <li>Adjust any other component settings as needed</li> </ol>"},{"location":"Guides/simple_vqa/#running-the-example","title":"Running the Example","text":"<ol> <li>Run the simple VQA example:</li> </ol> <p>For terminal/CLI usage:    <code>bash    python run_cli.py</code></p> <p>For app/GUI usage:    <code>bash    python run_app.py</code></p>"},{"location":"Guides/simple_vqa/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues: - Verify Redis is running and accessible - Check your OpenAI API key is valid - Ensure all dependencies are installed correctly - Review logs for any error messages</p>"},{"location":"Guides/simple_vqa/#building-the-example","title":"Building the Example","text":"<p>Coming soon! This section will provide detailed instructions for building and packaging the step1_simpleVQA example step by step.</p>"},{"location":"Guides/Let%20the%20agent%20remember/1_memory/","title":"Memory","text":"<p>How to use LTM memory.</p>"},{"location":"Guides/Let%20the%20agent%20think/1_workflow/","title":"More complex workflow","text":"<p>Advanced Node Concepts: Decider &amp; Loop</p>"},{"location":"Guides/Let%20the%20agent%20think/2_tool/","title":"Tools","text":"<p>How to let the agent use tools.</p>"},{"location":"core_concepts/container/","title":"Container","text":"<p>The Container module is a dependency injection and service container implementation that manages components and their dependencies in the OmAgent core system. It follows the IoC (Inversion of Control) pattern to handle component registration, configuration, and retrieval.</p>"},{"location":"core_concepts/container/#key-features","title":"Key Features","text":""},{"location":"core_concepts/container/#1-component-management","title":"1. Component Management","text":"<ul> <li>Registers and manages different types of components (connections, memories, callbacks, etc.)</li> <li>Handles component dependencies automatically</li> <li>Provides type-safe access to registered components</li> </ul>"},{"location":"core_concepts/container/#2-connector-management","title":"2. Connector Management","text":"<p>Manages service connectors that components might depend on - Automatically injects required connectors into components</p>"},{"location":"core_concepts/container/#3-special-component-types","title":"3. Special Component Types","text":"<ul> <li>STM (Short-term Memory)</li> <li>LTM (Long-term Memory)</li> <li>Callback handlers</li> <li>Input handlers</li> </ul>"},{"location":"core_concepts/container/#4-configuration-management","title":"4. Configuration Management","text":"<ul> <li>Can compile configurations to YAML</li> <li>Loads configurations from YAML files</li> <li>Supports environment variables and descriptions in configs</li> </ul>"},{"location":"core_concepts/container/#register","title":"Register","text":"<p>Examples of registering:</p> <pre><code>from omagent_core.utils.container import container\nfrom omagent_core.services.handlers.redis_stream_handler import RedisStreamHandler\n\n# Register a connector using component name\ncontainer.register_connector(RedisConnector, name=\"redis_client\")\n\n# Register a component using component class\ncontainer.register_component(RedisStreamHandler)\n\n# Register STM component\ncontainer.register_stm(\"RedisSTM\")\n\n# Register LTM component\ncontainer.register_ltm(\"MilvusLTM\")\n\n# Register callback and input handlers\ncontainer.register_callback(\"AppCallback\")\ncontainer.register_input(\"AppInput\")\n</code></pre>"},{"location":"core_concepts/container/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Compile Configuration: Container can automatically generate YAML configuration template files. You can change the values of the parameters in the template files which will take effect when loading the configuration. The <code>env_var</code> indicates the environment variable names for the parameters, don't change it because it is just for demonstration.    <code>python    from pathlib import Path    container.compile_config(Path('./config_dir'))</code></p> </li> <li> <p>Load Configuration: Load settings from YAML files. This will update the container with the settings in the YAML file.    <code>python    container.from_config('container.yaml')</code></p> </li> </ol>"},{"location":"core_concepts/container/#component-retrieval","title":"Component Retrieval","text":"<p>Access registered components:</p> <pre><code># Get a connector\nredis_client = container.get_connector(\"redis_client\")\n\n# Get STM component\nstm = container.stm\n\n# Get LTM component\nltm = container.ltm\n\n# Get callback handler\ncallback = container.callback\n\n# Get input handler\ninput_handler = container.input\n</code></pre>"},{"location":"core_concepts/container/#best-practices","title":"Best Practices","text":"<ol> <li>Early Registration: Register all components at application startup</li> <li>Configuration Files: Use YAML configuration files for better maintainability</li> <li>Compile Configuration: Prepare a separated script to compile container configuration before application startup. </li> <li>Update Container: Update the container with the settings in project entry file. Do register default Special Components (STM, LTM, Callback, Input) before update.</li> <li>Single Instance: Use the global container instance provided by the framework</li> </ol>"},{"location":"core_concepts/debug/","title":"Running In Debug Mode","text":"<p>Debug Mode: Set <code>debug: true</code> in the <code>conductor_config</code> section within <code>container.yaml</code> to enable debug mode. The debug mode has the following features: 1. Outputs more debug information. 2. After starting, it will stop all workflows with the same name on the conductor and restart a new workflow. 3. There will be no retries after failure, each task will only be executed once.</p>"},{"location":"core_concepts/registry/","title":"Register","text":"<p>The Registry module is a powerful tool for managing and organizing different types of modules in your application. It supports registration and retrieval of various categories like prompts, LLMs, workers, tools, encoders, connectors, and components.</p>"},{"location":"core_concepts/registry/#registration","title":"Registration","text":"<p>You can register classes using either decorators or direct registration:</p> <pre><code>from omagent_core.utils.registry import registry\n\n# Using decorator (recommended)\n@registry.register_node()\nclass MyNode:\n    name = \"MyNode\"\n\n# Or with a custom name\n@registry.register_tool(name=\"custom_tool_name\")\nclass MyTool:\n    pass\n\n# Direct registration\nclass MyLLM:\n    pass\nregistry.register(\"llm\", \"my_llm\")(MyLLM)\n</code></pre>"},{"location":"core_concepts/registry/#retrieval","title":"Retrieval","text":"<p>Retrieve registered modules using the get methods:</p> <pre><code># Get registered modules\nmy_node = registry.get_node(\"MyNode\")\nmy_tool = registry.get_tool(\"custom_tool_name\")\nmy_llm = registry.get(\"llm\", \"my_llm\")\n</code></pre>"},{"location":"core_concepts/registry/#auto-import-feature","title":"Auto-Import Feature","text":"<p>The registry can automatically import modules from specified paths:</p> <pre><code># Import from default paths\nregistry.import_module()\n\n# Import from custom project path\nregistry.import_module(\"path/to/your/modules\")\n\n# Import from multiple paths\nregistry.import_module([\n    \"path/to/modules1\",\n    \"path/to/modules2\"\n])\n</code></pre> <p>Note: Do use the <code>registry.import_module()</code> in the main function of your script so that the modules can be registered to python environment before being used.</p>"},{"location":"core_concepts/Clients/app/","title":"App","text":"<p>OmAgent App is an app for developers to visualize and edit business scenario content for large model capabilities. It supports interaction with multimodal large models by obtaining data from mobile phone cameras, audio streams, etc., and combining memory, tool invocation, and other capabilities. Based on the business scenarios developed by users, it outputs corresponding content through interaction with the Agent service, providing a demonstration app for intelligent agent scenario development with multimodal content input and output. Next, we will introduce the relevant functions of OmAgent App step by step to start the user's experience journey.</p>"},{"location":"core_concepts/Clients/app/#app-installation","title":"App Installation","text":"<p>The QR code for downloading the app is as follows:</p> <p></p> <ul> <li>Currently, only Android phones are supported for download and use, but iOS support is coming soon.</li> </ul>"},{"location":"core_concepts/Clients/app/#app-usage","title":"App Usage","text":""},{"location":"core_concepts/Clients/app/#1-app-home-page","title":"1. APP Home Page","text":"<p>After opening the APP, the guide page will be displayed as shown in the figure below:</p> <p></p> <p>The APP home page includes Mobile, Glasses, and Connection Settings as shown below:</p> <p></p>"},{"location":"core_concepts/Clients/app/#11-connection-settings","title":"1.1 Connection Settings","text":"<p>The app automatically searches for and connects to the environment IP running on the local network. If the connection is successful, a toast message will display: \"Connection Successful\". If the connection fails, a toast message will display: \"Service connection failure\". - Click the \"Connection Settings\" button at the bottom of the homepage to enter the app configuration page, as shown in the figure below;</p> <p></p> <ul> <li>Enter the correct IP in the IP input box and click the \"Connection\" button at the bottom. After a successful connection, return to the homepage and click the \"Mobile\" section to enter the Mobile page. </li> <li>Note: The IP input box displays the last successfully connected IP address by default.</li> </ul>"},{"location":"core_concepts/Clients/app/#2-mobile","title":"2. Mobile","text":"<p>\"Mobile\" mainly includes settings, voice input, camera, multimodal, brush functions, etc.</p>"},{"location":"core_concepts/Clients/app/#21-return-to-home-page","title":"2.1 Return to Home Page","text":"<p>Click the top left return to home button  to return to the home page.</p>"},{"location":"core_concepts/Clients/app/#22-settings","title":"2.2 Settings","text":"<p>Click the top right settings button , the page will pop up a settings window, as shown below. Click the  button or outside the window to close the window.</p> <p></p>"},{"location":"core_concepts/Clients/app/#221-album","title":"2.2.1 Album","text":"<p>Click Album to enter the gallery page, loading 80 images at a time, as shown below:</p> <p></p> <ul> <li> <p>Click Reindex, if indexing is successful, the page will prompt success, indicating that all images in the gallery have been successfully indexed. If it fails, it will prompt: failure. After selecting images, click Reindex to index only the selected images.</p> </li> <li> <p>Click Select to trigger batch selection, all images on the page will display a checkmark icon at the bottom right; the page selection button changes to deselect, the Upload button changes to Delete, allowing images to be selected for deletion; click Cancel to return to the unselected state.</p> </li> <li> <p>Click Upload to call the album, select images to upload, the gallery dynamically displays, uploads are displayed immediately, up to 20 images can be selected at a time. During the upload process, click Cancel to stop uploading images (the page only displays successfully uploaded images).</p> </li> <li> <p>Click on an image to enlarge and preview it, as shown below; supports left and right sliding and deletion. The top displays the image page number, which can be clicked to return to the gallery.</p> </li> </ul> <p></p> <ul> <li>Click the top left return button on the gallery page , the return button returns to the Mobile page, and the settings window is still displayed.</li> </ul>"},{"location":"core_concepts/Clients/app/#222-chat-history","title":"2.2.2 Chat History","text":"<p>Click Chat history to enter the history dialogue page, displaying all dialogue content, as shown below; click Delete all to confirm the operation and clear the history dialogue.</p> <p></p> <p>History dialogue display rules: - Different Workflow dialogues are displayed separated by time, supporting up and down sliding.</p> <ul> <li> <p>A single Workflow content dialogue includes text and images, with multiple image scenarios. Click on an image to preview it, and multiple image previews support sliding.</p> </li> <li> <p>The progress menu within a single Workflow supports clicking to expand, as shown below:</p> </li> </ul> <p></p> <ul> <li>Share: Click the share button under the Workflow dialogue, the share function is displayed at the bottom of the page, as shown below: sharing can be done as needed.</li> </ul> <p></p>"},{"location":"core_concepts/Clients/app/#223-multi-turn-dialogue","title":"2.2.3 Multi-turn Dialogue","text":"<p>Click Multi-turn dialogue to display the multi-turn dialogue dropdown menu, default is 1 turn, up to 10 turns can be set, as shown below:</p> <p></p>"},{"location":"core_concepts/Clients/app/#224-workflow-settings","title":"2.2.4 Workflow Settings","text":"<p>Click Workflow Settings to enter the Workflow list selection page, as shown below:</p> <p></p> <p>In the list, you can click to select the required Workflow, only single selection is supported. After selection, a checkmark  icon is displayed. Click the refresh button  to refresh the list.</p>"},{"location":"core_concepts/Clients/app/#225-parameter-settings","title":"2.2.5 Parameter Settings","text":"<p>Click Parameter Settings to enter the custom parameter settings page, as shown below:</p> <p></p> <p>Click +Add parameter, the page displays Parameter name\\value input boxes, as shown below; the input boxes support deletion, and the input box can add up to 20. After adding 20, there is no +Add parameter button. Click SAVE to save successfully (the input box content can be empty to save successfully).</p> <p></p>"},{"location":"core_concepts/Clients/app/#23-global-voice","title":"2.3 Global Voice","text":"<p> Default is on, click to turn off, click to switch to on prompt: Voice auto-play is on, switch to off prompt: Voice auto-play is off.</p>"},{"location":"core_concepts/Clients/app/#24-camera","title":"2.4 Camera","text":"<p> Default is rear, click to switch to front, the page previews the camera screen in real-time, page gestures: \u2460 click to focus \u2461 pinch to zoom, after zooming, a 1X button is displayed, click the 1X button to return to 1X zoom, the 1X button disappears.</p>"},{"location":"core_concepts/Clients/app/#25-voice-dialogue","title":"2.5 Voice Dialogue","text":"<p>Long press the voice button to trigger the voice recognition function, long press to speak and then release, the page displays the dialogue section, as shown below:</p> <p></p> <p>Note: When the model reply content has images, click the image to enlarge and preview.</p>"},{"location":"core_concepts/Clients/app/#251-expand","title":"2.5.1 Expand","text":"<p> Click the expand button to display the entire dialogue section, as shown below:</p> <p></p> <p>Click the collapse button at the top of the image  to collapse the dialogue.</p>"},{"location":"core_concepts/Clients/app/#252-workflow","title":"2.5.2 Workflow","text":"<p> is the Workflow name of the current dialogue. Click the button on the right  to display the Workflow progress, as shown below:</p> <p></p> <p>The progress menu supports expanding and collapsing.</p>"},{"location":"core_concepts/Clients/app/#253-regenerate","title":"2.5.3 Regenerate","text":"<p> The regenerate button is displayed under the nearest model reply content. Click to regenerate and overwrite the previous reply content.</p>"},{"location":"core_concepts/Clients/app/#254-voice-broadcast","title":"2.5.4 Voice Broadcast","text":"<p> The small voice button is displayed under the nearest model reply content. When voice is playing, it is displayed as . After the nearest model reply ends or is manually clicked, it is displayed as . After the nearest model reply ends, click this button to replay the nearest model reply content; - When the global voice button is on, voice broadcast is on by default. After turning off the global voice, there is no voice broadcast. At this time, click the small voice button to replay the nearest model reply content.</p>"},{"location":"core_concepts/Clients/app/#255-stop-generating","title":"2.5.5 Stop Generating","text":"<p> Stop generating, in the content output state, you can manually click to stop generating and interrupt the model reply.</p>"},{"location":"core_concepts/Clients/app/#26-voice-timing-mode","title":"2.6 Voice - Timing Mode","text":"<p>Long press the voice button to speak, do not release, slide up to timing  and then release, the page enters the timing mode frame extraction 5-second countdown , timing frame extraction mode, voice input cannot be clicked, the button is grayed out; after the countdown ends, the dialogue module is displayed to show the dialogue content; (during the 5-second countdown, click the countdown to end the timing frame extraction early).</p>"},{"location":"core_concepts/Clients/app/#27-cancel-sending","title":"2.7 Cancel Sending","text":"<p>Long press the voice button to speak, do not release, slide up to  and then release, cancel sending.</p>"},{"location":"core_concepts/Clients/app/#28-brush","title":"2.8 Brush","text":"<p> Default is grayed out, click to enable the brush function, the page displays brush color and eraser buttons as shown below:</p> <p></p> <p>Supports selecting different colors for brush annotation on the real-time camera preview page; click the eraser  to remove all brush annotations; click the brush button  to turn off the brush function (brush marks are displayed on the dialogue images using the brush in the history dialogue page).</p>"},{"location":"core_concepts/Clients/build/","title":"Build","text":"<p>How to build a client.</p>"},{"location":"core_concepts/Clients/client/","title":"Client","text":"<p>Currently, there are three clients: <code>DefaultClient</code>, <code>AppClient</code>, and <code>WebpageClient</code>.</p> <p><code>DefaultClient</code> is the default client used for interacting with users via the command line.  - The parameters of <code>DefaultClient</code> include <code>interactor</code>, <code>processor</code>, <code>config_path</code>, <code>workers</code>, and <code>input_prompt</code>. - Among them, either <code>interactor</code> or <code>processor</code> must be chosen to be passed in. <code>interactor</code> is the workflow used for interaction, and <code>processor</code> is the workflow used for image processing. - At least one of <code>config_path</code> and <code>workers</code> must be passed in, or both can be passed. <code>config_path</code> is the path to the worker configuration file, and <code>workers</code> is a list of <code>Worker</code> instances. - <code>input_prompt</code> is the prompt message for user input, which defaults to None. If you need to provide a prompt message after startup, you need to pass it in. Alternatively, you can set <code>input_prompt</code> in the <code>self.input.read_input()</code> method of your first worker node.</p> <p><code>AppClient</code> is used for interacting with users within an app. - The parameters of <code>AppClient</code> include <code>interactor</code>, <code>processor</code>, <code>config_path</code>, and <code>workers</code>. - Among them, either <code>interactor</code> or <code>processor</code> must be chosen to be passed in. <code>interactor</code> is the workflow used for interaction, and <code>processor</code> is the workflow used for image processing. - At least one of <code>config_path</code> and <code>workers</code> must be passed in, or both can be passed. <code>config_path</code> is the path to the worker configuration file, and <code>workers</code> is a list of <code>Worker</code> instances.</p> <p><code>WebpageClient</code> is a web page chat window implemented with gradio, which can be used for interaction. - The parameters of <code>WebpageClient</code> include <code>interactor</code>, <code>processor</code>, <code>config_path</code>, and <code>workers</code>. - Among them, either <code>interactor</code> or <code>processor</code> must be chosen to be passed in. - <code>interactor</code> is the workflow used for interaction, with a default port of 7860 after startup, and the access address is <code>http://127.0.0.1:7860</code>. - <code>processor</code> is the workflow used for image processing, with a default port of 7861 after startup, and the access address is <code>http://127.0.0.1:7861</code>. - At least one of <code>config_path</code> and <code>workers</code> must be passed in, or both can be passed. <code>config_path</code> is the path to the worker configuration file, and <code>workers</code> is a list of <code>Worker</code> instances.</p> <p>The input for <code>DefaultClient</code> uses <code>AppInput</code>, and the output uses <code>DefaultCallback</code>. The input for <code>AppClient</code> uses <code>AppInput</code>, and the output uses <code>AppCallback</code>. The input for <code>WebpageClient</code> uses <code>AppInput</code>, and the output uses <code>AppCallback</code>.</p> <p>When writing an agent worker, you don't need to worry about which one to use. Simply call <code>self.input.read_input()</code> and <code>self.callback.send_xxx()</code>. Depending on whether <code>DefaultClient</code> or <code>AppClient</code> or <code>WebpageClient</code> is instantiated, different input and output logic will be followed.</p> <p>The input has only one method: - <code>read_input(workflow_instance_id: str, input_prompt = \"\")</code>   - <code>workflow_instance_id</code> is the ID of the workflow instance.   - <code>input_prompt</code> is the information prompting the user on what to input, which can be empty.</p> <p>The callback has five methods: - <code>send_incomplete(agent_id, msg, took=0, msg_type=MessageType.TEXT.value, prompt_tokens=0, output_tokens=0, filter_special_symbols=True)</code> - <code>send_block(agent_id, msg, took=0, msg_type=MessageType.TEXT.value, interaction_type=InteractionType.DEFAULT.value, prompt_tokens=0, output_tokens=0, filter_special_symbols=True)</code> - <code>send_answer(agent_id, msg, took=0, msg_type=MessageType.TEXT.value, prompt_tokens=0, output_tokens=0, filter_special_symbols=True)</code></p> <ul> <li><code>send_incomplete</code> (the conversation content is not yet complete), <code>send_block</code> (a single conversation has ended, but the overall result is not finished), <code>send_answer</code> (the overall return is complete).</li> <li>The required parameters for these three methods are <code>agent_id</code> and <code>msg</code>. <code>agent_id</code> is the ID of the workflow instance, and <code>msg</code> is the message content.</li> <li><code>took</code>, <code>msg_type</code>, <code>interaction_type</code>, <code>prompt_tokens</code>, and <code>output_tokens</code> are optional parameters, chosen based on the actual situation.</li> <li><code>took</code> is the time consumed by the program, in seconds.</li> <li><code>msg_type</code> is the message type, with three options: <code>MessageType.TEXT.value</code>, <code>MessageType.IMAGE_URL.value</code>, <code>MessageType.IMAGE_BASE64.value</code>. The default is <code>MessageType.TEXT.value</code>.</li> <li><code>interaction_type</code> is the interaction type, with two options: <code>InteractionType.DEFAULT.value</code>, <code>InteractionType.INPUT.value</code>. The default is <code>InteractionType.DEFAULT.value</code>, which means doing nothing. <code>InteractionType.INPUT.value</code> means that after this message is output, user input is required.</li> <li><code>prompt_tokens</code> is the number of input tokens, and <code>output_tokens</code> is the number of output tokens.</li> <li><code>filter_special_symbols</code> is a boolean parameter, in <code>AppClient</code> it defaults to <code>True</code>, and special symbols such as <code>*</code>, <code>#</code>, <code>-</code> will be filtered out from the message content when the message type is <code>MessageType.TEXT.value</code>.</li> <li><code>send_incomplete</code> must be followed by a <code>send_block</code>.</li> <li> <p>The last message must be <code>send_answer</code>.</p> </li> <li> <p><code>info(agent_id, progress, message)</code></p> </li> <li> <p>The required parameters for the <code>info</code> method are <code>agent_id</code>, <code>progress</code>, and <code>message</code>. <code>agent_id</code> is the ID of the workflow instance, <code>progress</code> is the program name, and <code>message</code> is the progress information.</p> </li> <li> <p><code>error(agent_id, error_code, error_info, **kwargs)</code></p> </li> <li>The required parameters for the <code>error</code> method are <code>agent_id</code>, <code>error_code</code>, and <code>error_info</code>. <code>agent_id</code> is the ID of the workflow instance, <code>error_code</code> is the error code, and <code>error_info</code> is the error information.</li> </ul>"},{"location":"core_concepts/Clients/config/","title":"Configuration","text":"<p>How to configure the client.</p>"},{"location":"core_concepts/Clients/intro/","title":"Clients","text":"<p>What is client.</p>"},{"location":"core_concepts/Memory/build/","title":"Build","text":"<p>How to build a memory.</p>"},{"location":"core_concepts/Memory/config/","title":"Configuration","text":"<p>How to configure the memory.</p>"},{"location":"core_concepts/Memory/intro/","title":"Memory","text":"<p>OmAgent implements two types of memory systems:</p> <ol> <li>Short-Term Memory (STM)</li> <li>Temporary storage for workflow-specific data</li> <li>Implemented using Redis by default</li> <li>Useful for storing session/workflow state</li> <li> <p>Data is volatile and workflow-instance specific</p> </li> <li> <p>Long-Term Memory (LTM)</p> </li> <li>Persistent storage for long-term data</li> <li>Implemented using vector database</li> <li>Supports vector storage and similarity search</li> <li>Data persists across different workflow instances</li> </ol>"},{"location":"core_concepts/Models/build/","title":"Build","text":"<p>How to build a models.</p>"},{"location":"core_concepts/Models/config/","title":"Configuration","text":"<p>How to configure the models.</p>"},{"location":"core_concepts/Models/intro/","title":"Models","text":"<p>What is models</p>"},{"location":"core_concepts/Models/llms/","title":"LLMs","text":"<p>LLMs are the core components of Omagent. They are responsible for generating text via Large Language Models.</p> <p>It is constructed by following parts: - <code>BaseLLM</code>: The base class for all LLMs, it defines the basic properties and methods for all LLMs. - <code>BaseLLMBackend</code>: The enhanced class for better using LLMs, you can assemble specific LLMs with different prompt templates and output parsers. - <code>BasePromptTemplate</code>: The base class for all prompt templates, it defines the input variables and output parser for a prompt template. - <code>BaseOutputParser</code>: The base class for all output parsers, it defines how to parse the output of an LLM result.</p>"},{"location":"core_concepts/Models/llms/#prompt-template","title":"Prompt Template","text":"<p>This is a simple way to define a prompt template.</p> <pre><code>from omagent_core.models.llms.prompt.prompt import PromptTemplate\n\n# Define a system prompt template\nsystem_prompt = PromptTemplate.from_template(\"You are a helpful assistant.\", role=\"system\")\n# Define a user prompt template\nuser_prompt = PromptTemplate.from_template(\"Tell me a joke about {{topic}}\", role=\"user\")\n</code></pre> <p><code>topic</code> is a variable in the user prompt template, it will be replaced by the actual input value.</p>"},{"location":"core_concepts/Models/llms/#output-parser","title":"Output Parser","text":"<p>This is a simple way to define a output parser.</p> <pre><code>from omagent_core.models.llms.prompt.parser import StrParser\n\noutput_parser = StrParser()\n</code></pre> <p><code>StrParser</code> is a simple output parser that returns the output as a string.</p>"},{"location":"core_concepts/Models/llms/#get-llm-result","title":"Get LLM Result","text":"<p>This is a simple way to define a LLM request and get the result of an LLM. 1. The worker class should inherit from <code>BaseWorker</code> and <code>BaseLLMBackend</code>, and define the LLM model in the <code>prompts</code> and <code>llm</code> field. <code>OutputParser</code> is optional, if not defined, the default <code>StrParser</code> will be used. 2. Override the <code>_run</code> method to define the workflow logic.</p> <pre><code>def _run(self, *args, **kwargs):\n    payload = {\n        \"topic\": \"weather\"\n    }\n    # 1. use the `infer` method to get the LLM result\n    chat_complete_res = self.infer(input_list=[payload])[0][\"choices\"][0][\"message\"].get(\"content\")\n    # 2. use the `simple_infer` method to get the LLM result, it's a shortcut for the `infer` method\n    simple_infer_res = self.simple_infer(topic=\"weather\")[\"choices\"][0][\"message\"].get(\"content\")\n    content = chat_complete_res[0][\"choices\"][0][\"message\"].get(\"content\")\n    print(content)\n    return {'output': content}\n</code></pre> <p>For Multi-Modal LLMs, it's also simple and intuitive.</p> <pre><code>def _run(self, *args, **kwargs):\n    payload = {\n        \"topic\": [\"this image\", PIL.Image.Image object, ...]\n    }\n    chat_complete_res = self.infer(input_list=[payload])[0][\"choices\"][0][\"message\"].get(\"content\")\n    return {'output': chat_complete_res}\n</code></pre> <p>The order of prompts given to the LLM is consistent with the order of elements in the list of variables, resulting in an alternating pattern of text and images.</p>"},{"location":"core_concepts/Node/build/","title":"Build","text":"<p>How to build a node.</p>"},{"location":"core_concepts/Node/config/","title":"Configuration","text":"<p>How to configure the node.</p>"},{"location":"core_concepts/Node/intro/","title":"Node","text":"<p>What is node</p>"},{"location":"core_concepts/Tool/build/","title":"Build","text":"<p>How to build a tool.</p>"},{"location":"core_concepts/Tool/config/","title":"Configuration","text":"<p>How to configure the tool.</p>"},{"location":"core_concepts/Tool/intro/","title":"Tool","text":"<p>OmAgent's tool system is a robust and flexible framework that allows developers to create, configure, register, and invoke various tools seamlessly. Tools in OmAgent are modular components that perform specific tasks, enabling the intelligent agent to interact with different services and perform complex operations.  </p>"},{"location":"core_concepts/Tool/intro/#building-a-tool","title":"Building a Tool","text":""},{"location":"core_concepts/Tool/intro/#key-components-of-a-tool","title":"Key Components of a Tool","text":"<p>To create a new tool in OmAgent, you need to define a class that inherits from <code>BaseTool</code> or one of its subclasses. This class should implement the necessary methods to perform its intended functionality.</p> <ul> <li>Description: A string that describes what the tool does.</li> <li>Arguments Schema (<code>ArgSchema</code>): Defines the input parameters required by the tool.</li> <li>Execution Methods: </li> <li><code>_run</code>: Synchronous execution.</li> <li><code>_arun</code>: Asynchronous execution.</li> </ul>"},{"location":"core_concepts/Tool/intro/#input-parameters-schema","title":"Input Parameters Schema","text":"<p>Configuration involves defining the input parameters that the tool requires and any additional settings it might need. This is typically defined with your tool class in json format. There are four attributes in each argument in the <code>ArgSchema</code>: - <code>description</code>: A string that describes what the tool does. - <code>type</code>: The type of the argument. Support <code>string</code>, <code>integer</code>, <code>number</code>, <code>boolean</code>. - <code>enum</code>: A list of allowed values for the argument. - <code>required</code>: A boolean that indicates whether the argument is required</p> <p>Here is an example of the <code>ArgSchema</code> for a tool that performs web search:</p> <pre><code>ARGSCHEMA = {\n    \"search_query\": {\"type\": \"string\", \"description\": \"The search query.\"},\n    \"goals_to_browse\": {\n        \"type\": \"string\",\n        \"description\": \"What's you want to find on the website returned by search. If you need more details, request it in here. Examples: 'What is latest news about deepmind?', 'What is the main idea of this article?'\",\n    },\n    \"region\": {\n        \"type\": \"string\",\n        \"description\": \"The region code of the search, default to `en-US`. Available regions: `en-US`, `zh-CN`, `ja-JP`, `de-DE`, `fr-FR`, `en-GB`.\",\n        \"required\": True,\n    },\n    \"num_results\": {\n        \"type\": \"integer\",\n        \"description\": \"The page number of results to return, default is 1, maximum is 3.\",\n        \"required\": True,\n    },\n}\n</code></pre>"},{"location":"core_concepts/Tool/intro/#registering-a-tool","title":"Registering a Tool","text":"<p>Use the <code>registry.register_tool()</code> decorator to register your tool so that it can be instantiated when building a worker. See registry for more details about the registry system.</p>"},{"location":"core_concepts/Tool/intro/#tool-manager","title":"Tool Manager","text":"<p>The <code>ToolManager</code> class is responsible for managing and executing tools. It handles tool initialization, execution, and schema generation.</p>"},{"location":"core_concepts/Tool/intro/#initialization","title":"Initialization","text":"<p>You can initialize the <code>ToolManager</code> with multiple ways: - Initialize with a list of tool class names or instances or configurations.   <code>python   tool_manager = ToolManager(tools=[\"Calculator\"])   tool_manager = ToolManager(tools=[Calculator()])   tool_manager = ToolManager(tools=[{\"name\": \"Calculator\", \"description\": \"Calculator tool.\"}])</code> - Initialize with a dictionary of key-value pairs, where the key is the tool name and the value is the tool instance or configuration.</p> <pre><code>tool_manager = ToolManager(tools={\"my_calculator\": Calculator()})\ntool_manager = ToolManager(tools={\"my_calculator\": {\"name\": \"Calculator\", \"description\": \"Calculator tool.\"}})\n</code></pre> <p>Also, you can initialize the <code>ToolManager</code> with a yaml file. The ToolManager will be instantiated when building a worker.</p> <pre><code>tools:\n    - Calculator\n    - CodeInterpreter\n    - ReadFileContent\n    - WriteFileContent\n    - ShellTool\n    - name: WebSearch\n      bing_api_key: ${env|bing_api_key, microsoft_bing_api_key}\n      llm: ${sub|text_res}\n\n</code></pre> <p>If you want the ToolManger to decide which tool to use and generate the corresponding inputs, you should also provide a llm with prompts to the ToolManager.</p>"},{"location":"core_concepts/Tool/intro/#execution","title":"Execution","text":"<p>Tools can be invoked using the <code>ToolManager</code>. Here's how to execute a tool with a given tool name and arguments:</p> <pre><code>tool_manager = ToolManager()\nresult = tool_manager.execute(\"Calculator\", {\"code\": \"print(2 + 3)\"})\nprint(result)\n</code></pre> <p>The <code>ToolManager</code> will retrieve the corresponding tool, validate the input arguments and execute the tool. Another way to execute a tool is use the <code>execute_task</code> method. You can provide a task and let the ToolManager decide which tool to use and generate the corresponding inputs.</p> <pre><code>tool_manager = ToolManager()\nresult = tool_manager.execute_task(\"Calculate the result of 2 + 3.\")\n</code></pre>"},{"location":"core_concepts/Workflow/build/","title":"Build","text":"<p>How to build a workflow.</p>"},{"location":"core_concepts/Workflow/config/","title":"Configuration","text":"<p>How to configure the workflow.</p>"},{"location":"core_concepts/Workflow/intro/","title":"Workflow","text":"<p>What is workflow</p>"},{"location":"core_concepts/Workflow/task/","title":"Task","text":"<p>Task is the basic unit of building workflow. There are two types of tasks: simple task and operator.</p>"},{"location":"core_concepts/Workflow/task/#simple-task","title":"Simple Task","text":"<p>The functionality of simple task is defined by binding it to a worker. Here is an example of how to define a simple task:</p> <pre><code>from omagent_core.engine.worker.base import BaseWorker\nfrom omagent_core.engine.workflow.conductor_workflow import ConductorWorkflow\nfrom omagent_core.engine.workflow.task.simple_task import simple_task\nfrom omagent_core.utils.registry import registry\n\n# Define a worker\n@registry.register_worker()\nclass SimpleWorker(BaseWorker):\n    def _run(self, my_name: str):\n        return {}\n\n# Define a workflow\nworkflow = ConductorWorkflow(name='my_exp')\n\n# Define a simple task\ntask = simple_task(task_def_name='SimpleWorker', task_reference_name='ref_name', inputs={'my_name': workflow.input('my_name')})\n\nworkflow &gt;&gt; task\n</code></pre> <p>Specify the task definition name(<code>task_def_name</code>) and the task reference name(<code>task_reference_name</code>). The task definition name should be the name of the corresponding worker class. The task reference name is used to identify the task in the workflow. Specify the inputs of the task. Inputs may be either values or references to a workflow's initial inputs or the outputs of preceding tasks. See workflow for workflow details.</p>"},{"location":"core_concepts/Workflow/task/#operators","title":"Operators","text":"<p>Operators are the build-in tasks provided by the workflow engine. They handle the workflow control logic.</p>"},{"location":"core_concepts/Workflow/task/#1-switch-task","title":"1. Switch Task","text":"<p>Switch task is used to make a decision based on the value of a given field.</p> <pre><code>from omagent_core.engine.workflow.task.switch_task import SwitchTask\nfrom omagent_core.engine.worker.base import BaseWorker\nfrom omagent_core.engine.workflow.conductor_workflow import ConductorWorkflow\nfrom omagent_core.engine.workflow.task.simple_task import simple_task\nfrom omagent_core.utils.registry import registry\n\n@registry.register_worker()\nclass SimpleWorker1(BaseWorker):\n    def _run(self):\n        print('worker1')\n        return {}\n\n@registry.register_worker()\nclass SimpleWorker2(BaseWorker):\n    def _run(self):\n        print('worker2')\n        return {} \n\n@registry.register_worker()\nclass SimpleWorker3(BaseWorker):\n    def _run(self):\n        print('worker3')\n        return {} \n\nworkflow = ConductorWorkflow(name='switch_test')\n\n# Create some example tasks (replace with your actual tasks)\ntask1 = simple_task(task_def_name='SimpleWorker1', task_reference_name='ref_name1')\ntask2 = simple_task(task_def_name='SimpleWorker2', task_reference_name='ref_name2')\ntask3 = simple_task(task_def_name='SimpleWorker3', task_reference_name='ref_name3')\n\n# 1. Create a switch task with a value-based condition\nswitch = SwitchTask(\n    task_ref_name=\"my_switch\",\n    case_expression=workflow.input('switch_case_value'),  # This will evaluate the switch_case_value from workflow input\n)\n\n# 2. Add cases\nswitch.switch_case(\"w1\", [task1])\nswitch.switch_case(\"w2\", [task2])\n\n# 3. Add default case (optional)\nswitch.default_case([task3])\n\nworkflow &gt;&gt; switch\n\nworkflow.register(overwrite=True)\n</code></pre> <p>This will create a basic workflow with a switch task shown below. (You can check the workflow definition at Conductor UI default at http://localhost:5001/workflowDefs).</p> <p> </p> <p>You can also chaining the switch cases as follows:  </p> <pre><code>switch.switch_case(\"w1\", [task1]).switch_case(\"w2\", [task2]).default_case([task3])\n</code></pre>"},{"location":"core_concepts/Workflow/task/#2-fork-join-task","title":"2. Fork-Join Task","text":"<p>The fork-join task is used to execute multiple tasks in parallel.</p> <pre><code>from omagent_core.engine.workflow.task.fork_task import ForkTask\nfrom omagent_core.engine.worker.base import BaseWorker\nfrom omagent_core.engine.workflow.conductor_workflow import ConductorWorkflow\nfrom omagent_core.engine.workflow.task.simple_task import simple_task\nfrom omagent_core.utils.registry import registry\n\n\n@registry.register_worker()\nclass SimpleWorker1(BaseWorker):\n    def _run(self):\n        print(\"worker1\")\n        return {}\n\n\n@registry.register_worker()\nclass SimpleWorker2(BaseWorker):\n    def _run(self):\n        print(\"worker2\")\n        return {}\n\n\n@registry.register_worker()\nclass SimpleWorker3(BaseWorker):\n    def _run(self):\n        print(\"worker3\")\n        return {}\n\n\n# Create the main workflow\nworkflow = ConductorWorkflow(name=\"fork_join_test\")\n\n# Create tasks for parallel execution\ntask1 = simple_task(task_def_name=\"SimpleWorker1\", task_reference_name=\"parallel_task1\")\ntask2 = simple_task(task_def_name=\"SimpleWorker2\", task_reference_name=\"parallel_task2\")\ntask3 = simple_task(task_def_name=\"SimpleWorker3\", task_reference_name=\"parallel_task3\")\n\n# Create parallel execution paths\npath1 = [task1]  # First parallel path\npath2 = [task2]  # Second parallel path\npath3 = [task3]  # Third parallel path\n\n# Create the fork task with multiple parallel paths\nfork_task = ForkTask(\n    task_ref_name=\"parallel_execution\",\n    forked_tasks=[path1, path2, path3],\n    # The join will wait for the last task in each path\n    join_on=[\"parallel_task1\", \"parallel_task2\", \"parallel_task3\"]\n)\n\n# Add the fork task to the workflow\nworkflow.add(fork_task)\n\nworkflow.register(overwrite=True)\n</code></pre> <p>This will create a basic workflow with a fork-join task shown below.</p> <p> </p>"},{"location":"core_concepts/Workflow/task/#3-do-while-task","title":"3. Do-While Task","text":""},{"location":"core_concepts/Workflow/worker/","title":"Worker","text":"<p>Worker is the basic unit of computation in OmAgent. It is responsible for executing tasks and generating outputs.  </p>"},{"location":"core_concepts/Workflow/worker/#how-to-define-a-worker","title":"How to define a worker","text":"<p>The most basic worker can be created like this:</p> <pre><code>from omagent_core.engine.worker.base import BaseWorker\nfrom omagent_core.utils.registry import registry\n\n@registry.register_worker()\nclass MyWorker(BaseWorker):\n    def _run(self, *args, **kwargs):\n        # Implement your business logic here\n        return {\"result\": \"some_value\"}\n</code></pre> <p>By inheriting from <code>BaseWorker</code>, you can define your own worker. The worker will be registered with the name of the class. Normally, use <code>@registry.register_worker()</code> to register the worker so that it can build from configurations. See registry for more details.</p>"},{"location":"core_concepts/Workflow/worker/#1-parameter-handling","title":"1. Parameter Handling","text":"<p>You can define typed parameters that are json serializable, and return in key-value format:</p> <pre><code>@registry.register_worker()\nclass ParameterWorker(BaseWorker):\n    def _run(self, name: str, age: int):\n        # Parameters will be automatically extracted\n        return {\n            \"message\": f\"Hello {name}, you are {age} years old\"\n        }\n</code></pre>"},{"location":"core_concepts/Workflow/worker/#2-integration","title":"2. Integration","text":"<p>You can integrate workers with other libraries to extend the functionality. A most common case is to integrate with LLMs. Here is an example of how:</p> <pre><code>@registry.register_worker()\nclass LLMWorker(BaseLLMBackend, BaseWorker)\n    llm: OpenaiGPTLLM\n    output_parser: StrParser\n    prompts: List[PromptTemplate] = Field(\n        default=[\n            PromptTemplate.from_template(template=\"Your prompt here\")\n        ]\n    )\n\n    def _run(self, *args, **kwargs):\n        return self.simple_infer()\n</code></pre>"},{"location":"core_concepts/Workflow/worker/#3-configuration-fields","title":"3. Configuration Fields","text":"<p>You can configure worker behavior using Pydantic Fields to set default values:</p> <pre><code>@registry.register_worker()\nclass ConfigurableWorker(BaseWorker):\n    poll_interval: float = Field(default=100)  # Polling interval in milliseconds\n    domain: str = Field(default=None)          # Workflow domain\n    concurrency: int = Field(default=5)        # Concurrency level\n</code></pre> <p>Note: do not use <code>alias</code> in the field definition.</p>"},{"location":"core_concepts/Workflow/worker/#4-async-support","title":"4. Async Support","text":"<p>Workers can be asynchronous:</p> <pre><code>@registry.register_worker()\nclass AsyncWorker(BaseWorker):\n    async def _run(self, *args, **kwargs):\n        async def count_task(i):\n            await asyncio.sleep(1)\n            print(f'Task {i} completed!')\n            return i\n\n        tasks = [count_task(i) for i in range(10)]\n        results = await asyncio.gather(*tasks)\n        return {\"result\": \"async operation completed\"}\n</code></pre>"},{"location":"core_concepts/Workflow/worker/#configuration-and-build","title":"Configuration and build","text":"<p>Workers can be configured and built from YAML or JSON configuration files. You not only can set the parameters, but the recursive dependencies.</p>"},{"location":"core_concepts/Workflow/worker/#1-worker-configuration-structure","title":"1. Worker Configuration Structure","text":"<p>Here's the basic structure:</p> <pre><code>name: LLMWorker\nllm:\n    name: OpenaiGPTLLM\n    model_id: gpt-4o\n    api_key: sk-proj-...\n    endpoint: https://api.openai.com/v1\n    temperature: 0\n    vision: true\noutput_parser:\n    name: StrParser\n</code></pre>"},{"location":"core_concepts/Workflow/worker/#2-submodule-substitution","title":"2. Submodule Substitution","text":"<p>You can use the ${sub|module_name} to substitute submodules. This is useful when you want to reuse the same submodule in different workers and also keep the configuration clean. The module_name should be the name of the submodule configuration file. For example, you can define the llm_worker.yaml as follows:</p> <pre><code>name: LLMWorker\nllm: ${sub|gpt}\noutput_parser:\n    name: StrParser\n</code></pre> <p>And define the gpt.yaml as follows:</p> <pre><code>name: OpenaiGPTLLM\nmodel_id: gpt-4o\napi_key: sk-proj-...\nendpoint: https://api.openai.com/v1\ntemperature: 0\nvision: true\n</code></pre> <p>This is equivalent to the previous LLMWorker example. Note: Do not use <code>alias</code> in the field definition. Do not create Circular reference.</p>"},{"location":"core_concepts/Workflow/worker/#3-environment-variables","title":"3. Environment Variables","text":"<p>You can use the ${env|env_name, default_value} to substitute environment variables. This is useful when you want to set the parameters dynamically. The env_name should be the name of the environment variable. default_value is optional, and will be used when the environment variable is not set. For example, you can define the gpt.yaml as follows:</p> <pre><code>name: OpenaiGPTLLM\nmodel_id: gpt-4o\napi_key: ${env| CUSTOM_OPENAI_KEY}\nendpoint: ${env| CUSTOM_OPENAI_ENDPOINT, https://api.openai.com/v1}\ntemperature: 0\nvision: true\n</code></pre> <p>The environment variable name is case-sensitive.</p>"},{"location":"core_concepts/Workflow/worker/#4-default-configuration-fields","title":"4. Default Configuration Fields","text":"<p>Workers have several default configuration fields that can be set: - component_stm: The STM component for the worker. Use any registered component name. Default is the one registered with <code>register_stm</code>. Access it via <code>self.stm</code>. See container and memory for more details. - component_ltm: The LTM component for the worker. Use any registered component name. Default is the one registered with <code>register_ltm</code>. Access it via <code>self.ltm</code>. See container and memory for more details. - component_callback: The callback component for the worker. Use any registered component name. Default is the one registered with <code>register_callback</code>. Access it via <code>self.callback</code>. See container and client for more details. - component_input: The input component for the worker. Use any registered component name. Default is the one registered with <code>register_input</code>. Access it via <code>self.input</code>. See container and client for more details. - poll_interval: The poll interval for the worker. Default is 100 milliseconds. - domain: The domain of the workflow. Default is None. - concurrency: The concurrency of the worker. Default is 5.</p>"},{"location":"core_concepts/Workflow/worker/#5-build-from-configurations","title":"5. Build from Configurations","text":"<p>The worker instances can be built from configurations by using the <code>build_from_file</code> function from omagent_core.utils.build. Here's how it works:</p> <pre><code>from omagent_core.utils.build import build_from_file\n\n# Load worker configs from a directory\nworker_config = build_from_file('path/to/config/directory')\n</code></pre> <p>Note: You must provide a <code>workers</code> directory in the configuration path which contains all configurations for the workers. </p>"},{"location":"core_concepts/Workflow/worker/#run-workers","title":"Run workers","text":"<p>OmAgent provides a TaskHandler class to manage worker instance creation and management. Here's how to use TaskHandler:</p> <pre><code>from omagent_core.engine.automator.task_handler import TaskHandler\n\ntask_handler = TaskHandler(worker_config=worker_config, workers=[MyWorker()])\ntask_handler.start_processes()\ntask_handler.stop_processes()\n</code></pre> <p>The <code>worker_config</code> parameter accepts a set of worker configurations and launches the corresponding number of processes based on each worker's concurrency attribute value.  </p> <p>You can also use the <code>workers</code> parameter to directly pass in instantiated worker objects. Instances of these workers are deepcopied based on the concurrency setting. If your worker instances contain objects that cannot be deepcopied, set the instance's concurrency property to 1 and actively expand the concurrency count in the workers list.  </p> <p>Then, use <code>start_processes</code> to start all workers and <code>stop_processes</code> to stop all workers.</p>"},{"location":"core_concepts/Workflow/worker/#important-notes","title":"Important Notes","text":"<ul> <li>Always use the @registry.register_worker() decorator to register the worker</li> <li>The <code>_run</code> method is mandatory and contains your core logic</li> <li>Return values should be a dictionary with serializable values</li> <li>Worker behavior can be configured using Fields</li> <li>Both synchronous and asynchronous operations are supported</li> <li>The <code>self.workflow_instance_id</code> is automatically available in the worker context</li> </ul>"},{"location":"core_concepts/Workflow/worker/#best-practices","title":"Best Practices","text":"<ul> <li>Keep workers focused on a single responsibility</li> <li>Use proper type hints for better code clarity</li> <li>Implement proper error handling</li> <li>Document your worker's expected inputs and outputs</li> <li>Use configuration fields for flexible behavior</li> <li>Consider using async operations for I/O-bound tasks</li> </ul>"},{"location":"core_concepts/Workflow/workflow/","title":"Workflow","text":"<p>Workflow is the top-level object in Omagent. It contains a list of tasks and the dependencies between them.  </p>"},{"location":"core_concepts/Workflow/workflow/#creating-a-workflow","title":"Creating a Workflow","text":"<p>You can create a workflow by instantiating the <code>ConductorWorkflow</code> class.</p> <pre><code>from omagent_core.engine.workflow.conductor_workflow import ConductorWorkflow\n\nworkflow = ConductorWorkflow(name='test_workflow')\n</code></pre>"},{"location":"core_concepts/Workflow/workflow/#adding-tasks-to-a-workflow","title":"Adding Tasks to a Workflow","text":"<p>You can add tasks to a workflow by using <code>add</code> method. (See task for more details about tasks)</p> <pre><code>workflow.add(task)\n</code></pre> <p>There is a shortcut operator <code>&gt;&gt;</code> for this method.</p> <pre><code>workflow &gt;&gt; task\n</code></pre> <p>Also, you can chaining the tasks as follows:</p> <pre><code>workflow &gt;&gt; task1 &gt;&gt; task2 &gt;&gt; task3\n</code></pre> <p>There is a simple way to create fork-join tasks.</p> <pre><code>workflow &gt;&gt; task1 &gt;&gt; [task2, task3, task4] &gt;&gt; task5\n</code></pre> <p>There is also a simple way to define a switch task.</p> <pre><code>workflow &gt;&gt; switch_task &gt;&gt; {'case1': task1, 'case2': task2, 'default': task3} #  default is for a scenario that the result does not correspond to any specified case\n</code></pre> <p>Note that the switch_task MUST output <code>switch_case_value</code> as indicator for branching.</p> <p>You can use a workflow as a task in another workflow.</p> <pre><code>sub_workflow &gt;&gt; task1 &gt;&gt; task2\nworkflow &gt;&gt; task3 &gt;&gt; sub_workflow &gt;&gt; task4\n</code></pre>"},{"location":"core_concepts/Workflow/workflow/#registering-a-workflow","title":"Registering a Workflow","text":"<p>You can register a workflow by using <code>register</code> method.</p> <pre><code>workflow.register(overwrite=True)\n</code></pre> <p>After registering, you can see the workflow in the Conductor UI (default at http://localhost:5001/workflowDefs).</p>"},{"location":"core_concepts/Workflow/workflow/#running-a-workflow","title":"Running a Workflow","text":"<p>You can start a workflow instance and send input to it by using <code>start_workflow_with_input</code> method.</p> <pre><code>workflow_execution_id = workflow.start_workflow_with_input(workflow_input={'name': 'Lu'})\n</code></pre>"},{"location":"core_concepts/Workflow/workflow/#getting-workflow-status-and-result","title":"Getting Workflow Status and Result","text":"<p>Since the workflow is a async task, you can get its status and result by using <code>get_workflow</code> method.</p> <pre><code>status = workflow.get_workflow(workflow_id=workflow_execution_id).status\nresult = workflow.get_workflow(workflow_id=workflow_execution_id).output\n</code></pre>"},{"location":"getting_started/2_devices/","title":"Connect to device","text":"<p>How to connect to the app. Using the simplest visual Q&amp;A as an example, follow up on content 1 and connect the chatbot to the app.</p>"},{"location":"getting_started/install/","title":"Installation","text":""},{"location":"getting_started/install/#1-deploy-the-workflow-orchestration-engine","title":"1. Deploy the Workflow Orchestration Engine","text":"<p>OmAgent utilizes Conductor as its workflow orchestration engine. Conductor is an open-source, distributed, and scalable workflow engine that supports a variety of programming languages and frameworks. By default, it uses Redis for persistence and Elasticsearch (7.x) as the indexing backend. It is recommended to deploy Conductor using Docker:</p> <pre><code>docker-compose -f docker/conductor/docker-compose.yml up -d\n</code></pre> <ul> <li>Once deployed, you can access the Conductor UI at <code>http://localhost:5001</code>. (Note: Mac system will occupy port 5000 by default, so we use 5001 here. You can specify other ports when deploying Conductor.)</li> <li>The Conductor API can be accessed via <code>http://localhost:8080</code>.</li> <li>More details about the deployment can be found here.</li> </ul>"},{"location":"getting_started/install/#2-install-omagent","title":"2. Install OmAgent","text":"<ul> <li>Python Version: Ensure Python 3.10 or higher is installed.</li> <li>Install <code>omagent_core</code>:   <code>bash   pip install -e omagent-core</code></li> <li> <p>Install dependencies for the sample project:   <code>bash   pip install -r requirements.txt</code></p> </li> <li> <p>Install Optional Components: </p> </li> <li>Install Milvus VectorDB for enhanced support of long-term memory. OmAgent uses Milvus Lite as the default vector database for storing vector data related to long-term memory. To utilize the full Milvus service, you may deploy the Milvus vector database via Docker.</li> <li>Pull git lfs files. We provide sample image files for our examples in the <code>examples/step4_outfit_with_ltm/wardrobe_images</code> directory. To use them, ensure Git LFS is installed. You can install it with the following command:       <code>bash       git lfs install</code>       Then, pull the files by executing:       <code>bash       git lfs pull</code></li> </ul>"},{"location":"getting_started/install/#3-connect-devices","title":"3. Connect Devices","text":"<p>If you wish to use smart devices to access your agents, we provide a smartphone app and corresponding backend, allowing you to focus on agent functionality without worrying about complex device connection issues. - Deploy the app backend      The APP backend comprises the backend program, along with two middleware components: the MySQL database and MinIO object storage. For installation and deployment instructions, please refer to this link. - Download, install, and debug the smartphone app   At present, we offer an Android APP available for download and testing. For detailed instructions on acquiring and using it, please refer to here. The iOS version is currently under development and will be available soon.</p>"},{"location":"getting_started/intro/","title":"Introduction","text":"<p>OmAgent is an open-source agent framework designed to streamlines the development of on-device multimodal agents. Our goal is to enable agents that can empower various hardware devices, ranging from smart phone, smart wearables (e.g. glasses), IP cameras to futuristic robots. As a result, OmAgent creates an abstraction over various types of device and simplifies the process of connecting these devices to the state-of-the-art multimodal foundation models and agent algorithms, to allow everyone build the most interesting on-device agents. Moreover, OmAgent focuses on optimize the end-to-end computing pipeline, on in order to provides the most real-time user interaction experience out of the box. </p> <p>In conclusion, key features of OmAgent include:</p> <ul> <li> <p>Easy Connection to Diverse Devices: we make it really simple to connect to physical devices, e.g. phone, glasses and more, so that agent/model developers can build the applications that not running on web page, but running on devices. We welcome contribution to support more devices! </p> </li> <li> <p>Speed-optimized SOTA Mutlimodal Models: OmAgent integrates the SOTA commercial and open-source foundation models to provide application developers the most powerful intelligence. Moreover, OmAgent streamlines the audio/video processing and computing process to easily enable natural and fluid interaction between the device and the users. </p> </li> <li> <p>SOTA Multimodal Agent Algorithms: OmAgent provides an easy workflow orchestration interface for researchers and developers implement the latest agent algorithms, e.g. ReAct, DnC and more. We welcome contributions of any new agent algorithm to enable more complex problem solving abilities.</p> </li> <li> <p>Scalability and Flexibility: OmAgent provides an intuitive interface for building scalable agents, enabling developers to construct agents tailored to specific roles and highly adaptive to various applications.  </p> </li> </ul>"},{"location":"getting_started/intro/#architecture","title":"Architecture","text":"<p>The design architecture of OmAgent adheres to three fundamental principles: 1. Graph-based workflow orchestration;  2. Native multimodality;  3. Device-centricity.   </p> <p>With OmAgent, one has the opportunity to craft a bespoke intelligent agent program.  </p> <p>For a deeper comprehension of OmAgent, let us elucidate key terms:  </p> <p> </p> <ul> <li> <p>Devices: Central to OmAgent's vision is the empowerment of intelligent hardware devices through artificial intelligence agents, rendering devices a pivotal component of OmAgent's essence. By leveraging the downloadable mobile application we have generously provided, your mobile device can become the inaugural foundational node linked to OmAgent. Devices serve to intake environmental stimuli, such as images and sounds, potentially offering responsive feedback. We have evolved a streamlined backend process to manage the app-centric business logic, thereby enabling developers to concentrate on constructing the intelligence agent's logical framework.  See client for more details.</p> </li> <li> <p>Workflow: Within the OmAgent Framework, the architectural structure of intelligent agents is articulated through graphs. Developers possess the liberty to innovate, configure, and sequence node functionalities at will. Presently, we have opted for Conductor as the workflow orchestration engine, lending support to intricate operations like switch-case, fork-join, and do-while. See workflow for more details.</p> </li> <li> <p>Task and Worker: Throughout the OmAgent workflow development journey, Task and Worker stand as pivotal concepts. Worker embodies the actual operational logic of workflow nodes, whereas Task oversees the orchestration of the workflow's logic. Tasks are categorized into Operators, managing workflow logic (e.g., looping, branching), and Simple Tasks, representing nodes customized by developers. Each Simple Task is correlated with a Worker; when the workflow progresses to a given Simple Task, the task is dispatched to the corresponding worker for execution. See task and worker for more details.</p> </li> </ul>"},{"location":"getting_started/intro/#basic-principles-of-building-an-agent","title":"Basic Principles of Building an Agent","text":"<ul> <li> <p>Modularity: Break down the agent's functionality into discrete workers, each responsible for a specific task.</p> </li> <li> <p>Reusability: Design workers to be reusable across different workflows and agents.</p> </li> <li> <p>Scalability: Use workflows to scale the agent's capabilities by adding more workers or adjusting the workflow sequence.</p> </li> <li> <p>Interoperability: Workers can interact with various backends, such as LLMs, databases, or APIs, allowing agents to perform complex operations.</p> </li> <li> <p>Asynchronous Execution: The workflow engine and task handler manage the execution asynchronously, enabling efficient resource utilization.  </p> </li> </ul>"},{"location":"getting_started/quick_start/","title":"Quick Start","text":""},{"location":"getting_started/quick_start/#1configuration","title":"1\u3001Configuration","text":"<p>The container.yaml file is a configuration file that manages dependencies and settings for different components of the system. To set up your configuration:</p> <ol> <li> <p>Generate the container.yaml file:    <code>bash    cd examples/step2_outfit_with_switch    python compile_container.py</code>    This will create a container.yaml file with default settings under <code>examples/step2_outfit_with_switch</code>.</p> </li> <li> <p>Configure your LLM settings in <code>configs/llms/gpt.yml</code> and <code>configs/llms/text_res.yml</code>:</p> </li> <li> <p>Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file    <code>bash    export custom_openai_key=\"your_openai_api_key\"    export custom_openai_endpoint=\"your_openai_endpoint\"</code></p> </li> <li> <p>Update settings in the generated <code>container.yaml</code>:</p> <ul> <li>Configure Redis connection settings, including host, port, credentials, and both <code>redis_stream_client</code> and <code>redis_stm_client</code> sections.</li> </ul> </li> <li>Update the Conductor server URL under conductor_config section</li> <li> <p>Adjust any other component settings as needed</p> </li> <li> <p>Websearch uses duckduckgo by default. For better results, it is recommended to configure Bing Search by modifying the <code>configs/tools/websearch.yml</code> file and setting the <code>bing_api_key</code>.</p> </li> </ol> <p>For more information about the container.yaml configuration, please refer to the container module</p>"},{"location":"getting_started/quick_start/#2running-the-example","title":"2\u3001Running the Example","text":"<ol> <li>Run the outfit with switch example:</li> </ol> <p>For terminal/CLI usage: Input and output are in the terminal window    <code>bash    cd examples/step2_outfit_with_switch    python run_cli.py</code></p> <p>For app/GUI usage: Input and output are in the app    <code>bash    cd examples/step2_outfit_with_switch    python run_app.py</code>    For app backend deployment, please refer to here    For the connection and usage of the OmAgent app, please check app usage documentation</p>"}]}